{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPBFmGb5F1pCVk82Gy2GvQC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/9bishal/Learning_RAG/blob/main/RAG_Application_using_Langchain_Mistral_AI_and_Weviate_db_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hI0q_1ulX3-d"
      },
      "outputs": [],
      "source": [
        "!pip install weaviate-client langchain tiktoken pypdf rapidocr-onnxruntime"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "WEAVIATE_CLUSTER=\"https://n2ok5qpfrqwecijxzx1lba.c0.asia-southeast1.gcp.weaviate.cloud\"\n",
        "WEAVIATE_API_KEY=\"M3NvM2hnaHZ3Zk9TR3lqNV9ldXk4RkZGVTFNaUxST2M5bG9qSzU1VWwzb25MR09MYStyUHBTbTBVUGRNPV92MjAw\""
      ],
      "metadata": {
        "id": "RAFyL-RdbBBu"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import Weaviate\n",
        "import weaviate"
      ],
      "metadata": {
        "id": "rck707fhgTM4"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"weaviate-client>=3.26.7,<4.0.0\"\n"
      ],
      "metadata": {
        "id": "vEVnGKzdizpc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "WEAVIATE_URL=WEAVIATE_CLUSTER\n",
        "WEAVIATE_API_KEY = WEAVIATE_API_KEY\n",
        "client = weaviate.Client(\n",
        "    url=WEAVIATE_URL, auth_client_secret = weaviate.AuthApiKey(WEAVIATE_API_KEY)\n",
        ")"
      ],
      "metadata": {
        "id": "P27FDI0fYTRQ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain-community\n"
      ],
      "metadata": {
        "id": "szgIHWGeeUFL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda:\"UTF-8\""
      ],
      "metadata": {
        "id": "KM8AWlFdbM6E"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers"
      ],
      "metadata": {
        "id": "Bf7oHcq6kuN_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#specify embedding model\n",
        "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
        "embedding_model_name=\"sentence-transformers/all-mpnet-base-v2\"\n",
        "# model_kwargs={\"device\":\"cuda\"}\n",
        "embedding = HuggingFaceBgeEmbeddings(\n",
        "    model_name=embedding_model_name,\n",
        "    # model_kwargs =model_kwargs\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DCxYCxh4bilT",
        "outputId": "c808dc4b-1728-484e-ec81-9c92d57817a1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-893396662.py:5: LangChainDeprecationWarning: The class `HuggingFaceBgeEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  embedding = HuggingFaceBgeEmbeddings(\n",
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "loader = PyPDFLoader(\"/content/document_for_RAG.pdf\")\n",
        "pages = loader.load()\n"
      ],
      "metadata": {
        "id": "RUdxquC6eU6y"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pages"
      ],
      "metadata": {
        "id": "t_MLCMuhfVsr"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
        "docs = text_splitter.split_documents(pages)"
      ],
      "metadata": {
        "id": "A5njkAhIg1WL"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# docs"
      ],
      "metadata": {
        "id": "LzDRcYRCmckw"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# vector_db = Weaviate.from_documents(\n",
        "#     docs, embedding, client=client, by_text=False\n",
        "\n",
        "# )"
      ],
      "metadata": {
        "id": "ySLYuD0gmef3"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_property_names(doc):\n",
        "    new_metadata = {}\n",
        "    for key, value in doc.metadata.items():\n",
        "        # Replace invalid characters (like dots) with underscores\n",
        "        clean_key = key.replace(\".\", \"_\").replace(\"-\", \"_\")\n",
        "        new_metadata[clean_key] = value\n",
        "    doc.metadata = new_metadata\n",
        "    return doc\n",
        "\n",
        "# Apply cleaning to all docs\n",
        "docs = [clean_property_names(doc) for doc in docs]\n",
        "\n",
        "# Now store them in Weaviate\n",
        "vector_db = Weaviate.from_documents(\n",
        "    docs, embedding, client=client, by_text=False\n",
        ")\n"
      ],
      "metadata": {
        "id": "DRncIprFqflc"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R38c8ITAogVG"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\n",
        "    vector_db.similarity_search(\n",
        "    \"What is rag?\", k=3\n",
        ")[0].page_content\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gyobcS_9rCJj",
        "outputId": "56272b7b-25f6-42fa-ec51-084eeea9a858"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "documents for questions that are less likely to beneﬁt from retrieval, suggesting that null document\n",
            "mechanisms may not be necessary for RAG.\n",
            "G Parameters\n",
            "Our RAG models contain the trainable parameters for the BERT-base query and document encoder of\n",
            "DPR, with 110M parameters each (although we do not train the document encoder ourselves) and\n",
            "406M trainable parameters from BART-large, 406M parameters, making a total of 626M trainable\n",
            "18\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\n",
        "    vector_db.similarity_search(\n",
        "    \"What is rag?\", k=3\n",
        ")[1].page_content\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aORbYprZsX0F",
        "outputId": "6983c44c-dbce-4056-e103-c8cf55d2ad88"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cases for NQ, where an extractive model would score 0%.\n",
            "4.2 Abstractive Question Answering\n",
            "As shown in Table 2, RAG-Sequence outperforms BART on Open MS-MARCO NLG by 2.6 Bleu\n",
            "points and 2.6 Rouge-L points. RAG approaches state-of-the-art model performance, which is\n",
            "impressive given that (i) those models access gold passages with speciﬁc information required to\n",
            "generate the reference answer , (ii) many questions are unanswerable without the gold passages, and\n",
            "(iii) not all questions are answerable from Wikipedia alone. Table 3 shows some generated answers\n",
            "from our models. Qualitatively, we ﬁnd that RAG models hallucinate less and generate factually\n",
            "correct text more often than BART. Later, we also show that RAG generations are more diverse than\n",
            "BART generations (see §4.5).\n",
            "4.3 Jeopardy Question Generation\n",
            "Table 2 shows that RAG-Token performs better than RAG-Sequence on Jeopardy question generation,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\n",
        "    vector_db.similarity_search(\n",
        "    \"What is rag?\", k=3\n",
        ")[2].page_content\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J08ZMPAMqLpd",
        "outputId": "5b8d55d1-cd50-4e58-92a0-65645049649f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "and train RAG by directly minimizing the negative log-likelihood of answers. We compare RAG to\n",
            "the popular extractive QA paradigm [5, 7, 31, 26], where answers are extracted spans from retrieved\n",
            "documents, relying primarily on non-parametric knowledge. We also compare to “Closed-Book\n",
            "QA” approaches [52], which, like RAG, generate answers, but which do not exploit retrieval, instead\n",
            "relying purely on parametric knowledge. We consider four popular open-domain QA datasets: Natural\n",
            "Questions (NQ) [29], TriviaQA (TQA) [24]. WebQuestions (WQ) [3] and CuratedTrec (CT) [2]. As\n",
            "CT and WQ are small, we follow DPR [26] by initializing CT and WQ models with our NQ RAG\n",
            "model. We use the same train/dev/test splits as prior work [ 31, 26] and report Exact Match (EM)\n",
            "scores. For TQA, to compare with T5 [52], we also evaluate on the TQA Wiki test set.\n",
            "3.2 Abstractive Question Answering\n",
            "RAG models can go beyond simple extractive QA and answer questions with free-form, abstractive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from re import template\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "template=\"\"\"You are an assisitant for question-answering task.\n",
        "If you  don't know piece of retrieved context to answer the question.\n",
        "Use ten sentencese maximum and keep the answer concise.\n",
        "Question:{question}\n",
        "Context :{context}\n",
        "Answer:\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "4wdzcK8VsamS"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt =ChatPromptTemplate.from_template(template)"
      ],
      "metadata": {
        "id": "k-4NRL5QuGL7"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hkF10B_yuN5Y",
        "outputId": "42fbe670-c64a-4fc4-a9dc-77c05a30c769"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\"You are an assisitant for question-answering task.\\nIf you  don't know piece of retrieved context to answer the question.\\nUse ten sentencese maximum and keep the answer concise.\\nQuestion:{question}\\nContext :{context}\\nAnswer:\\n\\n\"), additional_kwargs={})])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.llms import HuggingFaceHub\n"
      ],
      "metadata": {
        "id": "xDuvxw4fuUNI"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "huggingface_hub_api_token=userdata.get('HUGGINGFACE_TOKEN_FOR_RAG')"
      ],
      "metadata": {
        "id": "6UFMgkqIzTci"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = HuggingFaceHub(\n",
        "    huggingface_api_token = huggingface_hub_api_token,\n",
        "    repo_id =\"mistralai/Mistral-7B-Instruct-v0.1\",\n",
        "    model_kwargs ={\"temperature\":1, \"max_length\":180}\n",
        ")"
      ],
      "metadata": {
        "id": "GM2mN3rwxr2t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "from langchain.schema.output_parser import StrOutputParser"
      ],
      "metadata": {
        "id": "AuyWV6qJ6jHp"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_parser=StrOutputParser()"
      ],
      "metadata": {
        "id": "PNMhzphf7Hrt"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever=vector_db.as_retriever()"
      ],
      "metadata": {
        "id": "5IeiTQ0L7Q0G"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rag_chain =(\n",
        "    {\"context\":retriever, \"question\":RunnablePassthrough()}\n",
        "    |prompt\n",
        "    |model\n",
        "    |output_parser\n",
        ")"
      ],
      "metadata": {
        "id": "mqmqofri6wSE"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rag_chain.invoke(\"what us RAG?\")"
      ],
      "metadata": {
        "id": "BHSCf4vj7YxR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dI8Bj_tg7gLL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}